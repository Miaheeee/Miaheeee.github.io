<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>A*</title>
    <url>/2023/07/09/A/</url>
    <content><![CDATA[<hr>
<h3 id="A-算法"><a href="#A-算法" class="headerlink" title="A*算法"></a>A*算法</h3><p>A* 算法维护一个优先级队列，结点的优先级由$f(n)$的值决定，$f(n)$值越小，结点的优先级越高。$f(n) = g(n)+h(n)$，其中$g(n)$为结点从起始状态到当前状态所花费的代价，$h(n)$为当前状态到终止状态距离的估计代价，为启发式函数。当$h(n)$小于等于结点n到终止状态的实际代价，则A* 算法一定能找到最优路径。</p>
<p>以下为 A* 的伪代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">初始化优先级队列OPEN，CLOSE<span class="token punctuation">(</span>OPEN里为待展开的结点，CLOSE中表示已经展开的结点<span class="token punctuation">)</span>
将初始状态加入OPEN
<span class="token keyword">while</span> OPEN<span class="token punctuation">:</span>
	n⬅OPEN中弹出优先级最高的结点
	将n加入CLOSE
	如果n为终止结点<span class="token punctuation">:</span>
		找n的parent，直到起始状态
		<span class="token keyword">return</span>
	遍历n的邻近结点m<span class="token punctuation">:</span>
		如果m在CLOSE中<span class="token punctuation">:</span>
			<span class="token keyword">continue</span>
		如果m也不在OPEN中<span class="token punctuation">:</span>
			设置m的parent为n
			将m加入OPEN中<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Q-森林宝石的秘密通道"><a href="#Q-森林宝石的秘密通道" class="headerlink" title="Q: 森林宝石的秘密通道"></a>Q: 森林宝石的秘密通道</h3><p><strong>题目描述：</strong></p>
<p>在一个神秘的森林里，有一块由9个小方格组成的魔法石板。石板上有8个宝石，每个宝石上刻有1-8中的一个数字(每个数字都不重复）。石板上还有一个空位，用0表示。通过移动空位周围的宝石，你可以改变宝石的位置。传说中，当宝石按照某个特定的顺序排列时(本题设为135702684)，魔法石板将会显露出通往一个宝藏的秘密通道。<br>现在，你站在这块魔法石板前，需要找到一种最少步骤的移动方法，将宝石排列成目标顺序。为了解开这个谜题，请使用A*算法来设计一个程序，帮助你从初始状态成功解锁秘密通道。</p>
<p><strong>要求：要求只能用A*算法。</strong></p>
<p><strong>输入格式：</strong>一行介个数字，空格用0表示，除0之外，分别表示从左到右从上到下的对应宝石上的数字。</p>
<p><strong>输入格式：</strong>只有一行，该行只有一个数字，表示从初始状态到目标状态需要的最少移动次数(测试数据中无特殊无法到达目标状态数据)。</p>
<p><strong>输入输出样例：</strong></p>
<p>输入：150732684</p>
<p>输出：2</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 该函数返回启发式函数的值，current_node为当前结点矩阵，goal_node为目标结点矩阵</span>
<span class="token keyword">def</span> <span class="token function">heuristic_f</span><span class="token punctuation">(</span>current_node<span class="token punctuation">,</span> goal_node<span class="token punctuation">)</span><span class="token punctuation">:</span>
    h <span class="token operator">=</span> <span class="token number">0</span>
    rows <span class="token operator">=</span> current_node<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    cols <span class="token operator">=</span> current_node<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>rows<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>cols<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> current_node<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">!=</span> goal_node<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token keyword">and</span> current_node<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">!=</span><span class="token number">0</span><span class="token punctuation">:</span>
                goal_x <span class="token operator">=</span> np<span class="token punctuation">.</span>argwhere<span class="token punctuation">(</span>goal_node<span class="token operator">==</span>current_node<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
                goal_y <span class="token operator">=</span> np<span class="token punctuation">.</span>argwhere<span class="token punctuation">(</span>goal_node<span class="token operator">==</span>current_node<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
                h <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token builtin">abs</span><span class="token punctuation">(</span>i<span class="token operator">-</span>goal_x<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>j<span class="token operator">-</span>goal_y<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> h<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>将open_list定义为一个按照f升序排列的列表，其中为待展开的结点，close_list中存储已经展开的结点。当open_list不为空时，每次弹出第一个结点。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 每个结点用一个字典表示</span>
    start <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    input_order <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 初始化3*3的输入矩阵</span>
    input_order <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> input_order<span class="token punctuation">]</span>
    start_node <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>input_order<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
    start<span class="token punctuation">[</span><span class="token string">'node'</span><span class="token punctuation">]</span> <span class="token operator">=</span> start_node
    start<span class="token punctuation">[</span><span class="token string">'parent'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    start<span class="token punctuation">[</span><span class="token string">'steps'</span><span class="token punctuation">]</span> <span class="token operator">=</span> get_steps<span class="token punctuation">(</span>start<span class="token punctuation">[</span><span class="token string">'node'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    start<span class="token punctuation">[</span><span class="token string">'g'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
    start<span class="token punctuation">[</span><span class="token string">'h'</span><span class="token punctuation">]</span> <span class="token operator">=</span> heuristic_f<span class="token punctuation">(</span>start<span class="token punctuation">[</span><span class="token string">'node'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> goal<span class="token punctuation">[</span><span class="token string">'node'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    start<span class="token punctuation">[</span><span class="token string">'f'</span><span class="token punctuation">]</span> <span class="token operator">=</span> start<span class="token punctuation">[</span><span class="token string">'g'</span><span class="token punctuation">]</span> <span class="token operator">+</span> start<span class="token punctuation">[</span><span class="token string">'h'</span><span class="token punctuation">]</span>
    <span class="token comment"># openlist中存储待展开的结点，closelist中存储已经展开的节点</span>
    openlist <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    closelist <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    openlist<span class="token punctuation">.</span>append<span class="token punctuation">(</span>start<span class="token punctuation">)</span>
    <span class="token keyword">while</span> openlist<span class="token punctuation">:</span>
        current <span class="token operator">=</span> openlist<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        closelist<span class="token punctuation">.</span>append<span class="token punctuation">(</span>current<span class="token punctuation">)</span>
        <span class="token comment"># 如果当前展开的结点与目标结点一致，则返回</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>current<span class="token punctuation">[</span><span class="token string">'node'</span><span class="token punctuation">]</span> <span class="token operator">==</span> goal<span class="token punctuation">[</span><span class="token string">'node'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>current<span class="token punctuation">[</span><span class="token string">'g'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            p_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            p <span class="token operator">=</span> current
            <span class="token keyword">while</span> p<span class="token punctuation">:</span>
                p_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>p<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'node'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                p <span class="token operator">=</span> p<span class="token punctuation">[</span><span class="token string">'parent'</span><span class="token punctuation">]</span>
            p_list<span class="token punctuation">.</span>reverse<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># print(p_list)</span>
            <span class="token keyword">return</span>
        children <span class="token operator">=</span> get_children<span class="token punctuation">(</span>current<span class="token punctuation">)</span>
        <span class="token keyword">for</span> child <span class="token keyword">in</span> children<span class="token punctuation">:</span>
            <span class="token comment"># 判断该结点是否出现和待展开的结点列表和已经展开的结点列表</span>
            exist_open <span class="token operator">=</span> <span class="token boolean">False</span>
            exist_close <span class="token operator">=</span> <span class="token boolean">False</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>openlist<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> <span class="token punctuation">(</span>child<span class="token punctuation">[</span><span class="token string">'node'</span><span class="token punctuation">]</span> <span class="token operator">==</span> openlist<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'node'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    exist_open <span class="token operator">=</span> <span class="token boolean">True</span>
                    <span class="token comment"># 如果该孩子结点的f小于与它展开相等的在openlist结点的值，则用该孩子结点替代</span>
                    <span class="token keyword">if</span> child<span class="token punctuation">[</span><span class="token string">'f'</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> openlist<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'f'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
                        <span class="token keyword">del</span> openlist<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
                        openlist<span class="token punctuation">.</span>append<span class="token punctuation">(</span>child<span class="token punctuation">)</span>
                        <span class="token keyword">break</span>
            <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>closelist<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> <span class="token punctuation">(</span>child<span class="token punctuation">[</span><span class="token string">'node'</span><span class="token punctuation">]</span> <span class="token operator">==</span> closelist<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'node'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    exist_close <span class="token operator">=</span> <span class="token boolean">True</span>
                    <span class="token keyword">break</span>
            <span class="token keyword">if</span> exist_open <span class="token operator">==</span> <span class="token boolean">False</span> <span class="token keyword">and</span> exist_close <span class="token operator">==</span> <span class="token boolean">False</span><span class="token punctuation">:</span>
                openlist<span class="token punctuation">.</span>append<span class="token punctuation">(</span>child<span class="token punctuation">)</span>
        openlist <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>openlist<span class="token punctuation">,</span> key<span class="token operator">=</span>itemgetter<span class="token punctuation">(</span><span class="token string">'f'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/07/01/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>中文纠错</title>
    <url>/2023/07/13/%E4%B8%AD%E6%96%87%E7%BA%A0%E9%94%99/</url>
    <content><![CDATA[<h2 id="一、任务"><a href="#一、任务" class="headerlink" title="一、任务"></a>一、任务</h2><p>中文拼写检测（Chinese Spelling Check ）旨在将句子中的错误拼写检测出来并进行纠正。这个任务是自然处理领域一个重要的任务。比如：当新闻中出现一些拼写错误或者不合适的词语使用，这个任务能帮助自动检测。常见的可能出现的错误包括相似字形出错、相似语义混淆出错等。</p>
<p>输入：一个包含错误的自然语言句子</p>
<p>输出：一个正确的自然语言句子</p>
<h2 id="二、实现过程"><a href="#二、实现过程" class="headerlink" title="二、实现过程"></a>二、实现过程</h2><h3 id="2-1-seq2seq（加入注意力机制和中文预训练模型初始化的embedding）"><a href="#2-1-seq2seq（加入注意力机制和中文预训练模型初始化的embedding）" class="headerlink" title="2.1 seq2seq（加入注意力机制和中文预训练模型初始化的embedding）"></a>2.1 seq2seq（加入注意力机制和中文预训练模型初始化的embedding）</h3><p>使用eq2seq模型，将文本纠错任务转换为翻译任务，即输入带有错字的序列，输出纠正后的序列。</p>
<p>首先输入文本序列经过编码器变换为一个定长的背景变量，作为解码器的初始隐藏状态，该背景变量中包含了输入序列的所有编码信息，在该实验中，背景变量取值为输入序列有效长度最后一个隐藏状态。在给定框架的基础上，将编码器由LSTM变为双向的GRU，主要原因由两点：一是GRU只含有两个门控结构，在超参数全部调优的情况下，GRU的参数比LSTM少，计算速度快，二是采用双向的GRU，每个时间步的隐藏状态不再只取决于该时间步之前的状态，而是同时取决于该时间步的之前和之后的子序列，因此最后选用的背景变量的编码信息会更丰富。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>embedding_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>解码器同样使用GRU，训练时，使用强制学习，解码器使用输入句子的编码信息和上个时间步的真实标签以及隐藏状态作为输入，在测试时，解码器使用输入句子的编码信息和当前时间步的输出以及隐藏状态作为输入。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding_size<span class="token punctuation">,</span>
                              self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h4 id="2-1-1-加入注意力机制"><a href="#2-1-1-加入注意力机制" class="headerlink" title="2.1.1 加入注意力机制"></a>2.1.1 加入注意力机制</h4><p>该实验中，在解码器中加入注意力机制，解码器在每一时间步调整对编码器所有时间步的隐藏状态做加权的权重，从而在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_hidden_size<span class="token punctuation">,</span> decoder_hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Attention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>enc_hidden_size <span class="token operator">=</span> encoder_hidden_size
        self<span class="token punctuation">.</span>dec_hidden_size <span class="token operator">=</span> decoder_hidden_size
        self<span class="token punctuation">.</span>fc_in <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
            encoder_hidden_size<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> decoder_hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc_out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
            encoder_hidden_size<span class="token operator">*</span><span class="token number">2</span> <span class="token operator">+</span> decoder_hidden_size<span class="token punctuation">,</span> decoder_hidden_size<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> output<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># output [batch, target_len, dec_hidden_size]</span>
        <span class="token comment"># context [batch, source_len, enc_hidden_size*2]</span>
        batch_size <span class="token operator">=</span> output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        y_len <span class="token operator">=</span> output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        x_len <span class="token operator">=</span> context<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># [batch_size * x_sentence_len, enc_hidden_size*2]</span>
        x <span class="token operator">=</span> context<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token operator">*</span>x_len<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># [batch_size * x_len, dec_hidden_size]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_in<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  
        <span class="token comment"># [batch_size, x_sentence_len, dec_hidden_size]</span>
        context_in <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> x_len<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># [batch_size, y_sentence_len, x_sentence_len]</span>
        atten <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>output<span class="token punctuation">,</span> context_in<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># [batch_size, y_sentence_len, x_sentence_len]</span>
        atten <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>atten<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># [batch_size, y_sentence_len, enc_hidden_size*2]</span>
        context <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>atten<span class="token punctuation">,</span> context<span class="token punctuation">)</span>
        <span class="token comment"># [batch_size, y_sentence_len, enc_hidden_size*2+dec_hidden_size]</span>
        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>context<span class="token punctuation">,</span> output<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># [batch_size * y_sentence_len, enc_hidden_size*2+dec_hidden_size]</span>
        output <span class="token operator">=</span> output<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token operator">*</span>y_len<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc_out<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># [batch_size, y_sentence_len, dec_hidden_size]</span>
        output <span class="token operator">=</span> output<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> y_len<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> atten<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="2-1-2-加入中文预训练模型初始化的embedding"><a href="#2-1-2-加入中文预训练模型初始化的embedding" class="headerlink" title="2.1.2 加入中文预训练模型初始化的embedding"></a>2.1.2 加入中文预训练模型初始化的embedding</h4><p>预训练词嵌入在大数据集上训练时捕获单词的语义和句法意义，它们能够提高自然语言处理模型的性能，故考虑将原本模型中的embedding初始化替换为“bert-base-chinese”预训练模型的embedding。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>in_tok_embed <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>out_tok_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding_size<span class="token punctuation">,</span> dataset_size<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>out_tok_embed<span class="token punctuation">.</span>weight <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>
    self<span class="token punctuation">.</span>in_tok_embed<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>但是由实验结果发现，用大型预训练模型初始化本实验的embedding，检测错误位置的f1值和纠正错字的f1值反而变差了，考虑原因可能是本实验的数据集过小，导致预测效果不好。</p>
<h3 id="2-2-Bert"><a href="#2-2-Bert" class="headerlink" title="2.2 Bert"></a>2.2 Bert</h3><p>复现论文<strong>Spelling Error Correction with Soft-Masked BERT</strong>，由于BERT模型以15%的概率mask句中的每个字，因此直接用BERT模型倾向于不做任何改动直接输出相同的句子，故而Soft-Masked BERT提出使用两个网络结构，先通过纠错网络得到错误的分布，然后在同纠错网络纠错。</p>
<h4 id="2-2-1-数据预处理"><a href="#2-2-1-数据预处理" class="headerlink" title="2.2.1 数据预处理"></a>2.2.1 数据预处理</h4><p>使用预训练模型的作为词典库（dataset）。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pretrained_tokenzier <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bert-base-chinese"</span><span class="token punctuation">)</span>
dataset <span class="token operator">=</span> pretrained_tokenzier<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h4 id="2-2-2使用bert-base-chinese预训练模型"><a href="#2-2-2使用bert-base-chinese预训练模型" class="headerlink" title="2.2.2使用bert-base-chinese预训练模型"></a>2.2.2<strong>使用bert-base-chinese预训练模型</strong></h4><p>首先将输入序列BERT模型，通过训练得到的词嵌入向量表示，可以认为其即代表单词本身及其含义。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            p_ <span class="token operator">=</span> self<span class="token punctuation">.</span>em_bert<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>input_mask<span class="token punctuation">)</span><span class="token punctuation">[</span>
                <span class="token string">'last_hidden_state'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>将上述得到输出输入到检测网络（detector），检测网络由一个双向GRU、一个线性函数和一个sigmoid非线性激活构成，最终得到每个字的错误概率。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BiGRU</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embedding_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> n_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>BiGRU<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embedding_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_layers<span class="token operator">=</span>n_layers<span class="token punctuation">,</span>
                          bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        gru_out<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        pi <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>gru_out<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> pi<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>每个词的最终的词嵌入表示由[‘mask’]的embedding表示以及本身的embedding加权平均表示构成，权重分别为p和（1-p）。p是检测网络的输出，p越大表示该字越有可能是错字，即[‘mask’]embed的权重越大。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">e_ <span class="token operator">=</span> p <span class="token operator">*</span> self<span class="token punctuation">.</span>mask_e <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>p<span class="token punctuation">)</span> <span class="token operator">*</span> e_bert<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>纠错网络（corrector）是一个BERT模型加上一个残差结构，即将BERT模型的输出加上字原本的embedding在进行softmax计算，最终取预测到字典中概率最大的字的index作为该字的预测输出。以下是代码详细构建过程。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">h <span class="token operator">=</span> self<span class="token punctuation">.</span>corrector<span class="token punctuation">(</span>e_<span class="token punctuation">,</span>
                           attention_mask<span class="token operator">=</span>encoder_extended_attention_mask<span class="token punctuation">,</span>
                           head_mask<span class="token operator">=</span>head_mask<span class="token punctuation">,</span>
                           encoder_hidden_states<span class="token operator">=</span>encoder_hidden_states<span class="token punctuation">,</span>
                           encoder_attention_mask<span class="token operator">=</span>encoder_extended_attention_mask<span class="token punctuation">)</span>
h <span class="token operator">=</span> h<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> e
h <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>h<span class="token punctuation">)</span>
out <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>h<span class="token punctuation">)</span>
label <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>input_ids <span class="token operator">!=</span> answer_input_ids<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
p <span class="token operator">=</span> p<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> p<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> self<span class="token punctuation">.</span>count_loss<span class="token punctuation">(</span>out<span class="token punctuation">,</span> answer_input_ids<span class="token punctuation">,</span> p<span class="token punctuation">,</span> label<span class="token punctuation">)</span>
decode_result <span class="token operator">=</span> out<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>损失值的计算是由检测网络和纠错网络的损失加权平均作为整个网络的损失，检测网络的损失函数是二分类交叉熵损失，纠错网络采用多分类交叉熵损失，权重分别为（1-gama）和gama，gama为0-1之间的小数，通常设置gama大于0.5，因为多分类比二分类问题复杂。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">count_loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_v<span class="token punctuation">,</span> true_v<span class="token punctuation">,</span> p<span class="token punctuation">,</span> true_label<span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss_d <span class="token operator">=</span> self<span class="token punctuation">.</span>criterion_d<span class="token punctuation">(</span>p<span class="token punctuation">,</span> true_label<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        loss_c <span class="token operator">=</span> self<span class="token punctuation">.</span>criterion_c<span class="token punctuation">(</span>out_v<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> true_v<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>gama <span class="token operator">*</span> loss_c <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>self<span class="token punctuation">.</span>gama<span class="token punctuation">)</span> <span class="token operator">*</span> loss_d
        <span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h4 id="2-2-2使用shibing624-x2F-macbert4csc-base-chinese预训练模型"><a href="#2-2-2使用shibing624-x2F-macbert4csc-base-chinese预训练模型" class="headerlink" title="2.2.2使用shibing624/macbert4csc-base-chinese预训练模型"></a>2.2.2<strong>使用shibing624/macbert4csc-base-chinese预训练模型</strong></h4><p>模型下载地址：<a href="https://huggingface.co/shibing624/macbert4csc-base-chinese">shibing624/macbert4csc-base-chinese · Hugging Face</a></p>
<p>在本实验中，我已经将该模型下载后放在10205501458/gectoolkit/properties/model/RNN文件夹下</p>
<p>修改softmaskedbert的模型结构：</p>
<p>检测网络不再使用bert.embedding，而是使用bert全部模型，并取消其梯度，防止参数过导致过拟合，将结果输入线性层之后sigmoid激活。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            e_bert <span class="token operator">=</span> self<span class="token punctuation">.</span>em_bert<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>input_mask<span class="token punctuation">)</span><span class="token punctuation">[</span>
                <span class="token string">'last_hidden_state'</span><span class="token punctuation">]</span>
 p <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>detector<span class="token punctuation">(</span>e_bert<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>同样，纠错网络也是使用整个bert模型的最后一层隐藏状态作为输入。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">h <span class="token operator">=</span> self<span class="token punctuation">.</span>corrector<span class="token punctuation">(</span>e_bert<span class="token punctuation">,</span>
attention_mask<span class="token operator">=</span>encoder_extended_attention_mask<span class="token punctuation">,</span>
head_mask<span class="token operator">=</span>head_mask<span class="token punctuation">,</span>
encoder_hidden_states<span class="token operator">=</span>encoder_hidden_states<span class="token punctuation">,</span>
encoder_attention_mask<span class="token operator">=</span>input_mask<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>损失函数不变，同上softmaskedbert。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>总结与反思</title>
    <url>/2023/07/14/%E6%80%BB%E7%BB%93%E4%B8%8E%E5%8F%8D%E6%80%9D/</url>
    <content><![CDATA[<p>博客功能实现及其技术选择</p>
<p>博客样式设计及其美学考量</p>
<p>博客制作过程中遇到的问题及其解决方法</p>
<h2 id="一、博客主题"><a href="#一、博客主题" class="headerlink" title="一、博客主题"></a>一、博客主题</h2><p>博客主题：技术总结</p>
<p>选取原因：一方面是因为写技术博客是一个将所学知识内化之后输出的过程，在此过程中可以加深对技术的理解；另一方面是想通过博客进行一些知识的分享。</p>
<h2 id="二、博客页面布局"><a href="#二、博客页面布局" class="headerlink" title="二、博客页面布局"></a>二、博客页面布局</h2><p>本博客主要分为以下几个主要界面：</p>
<ol>
<li>主页：背景全屏平铺，滚动字幕，往下滑动即可查阅所有博客</li>
<li>归档：包括所有博客，仅显示文章标题和写作日期</li>
<li>分类：分类文章</li>
<li>标签：文章中所带有的标签</li>
<li>友联：一些友情链接</li>
<li>关于我：博客作者介绍</li>
</ol>
<h2 id="三、博客功能实现及其技术选择"><a href="#三、博客功能实现及其技术选择" class="headerlink" title="三、博客功能实现及其技术选择"></a>三、博客功能实现及其技术选择</h2><p>本博客网站采用Hexo框架搭建，选取该框架的主要原因有以下几点：</p>
<ul>
<li><p><strong>渲染速度块</strong></p>
<p>Node.js 所带来的超快生成速度，让上百个页面在几秒内瞬间完成渲染。</p>
</li>
<li><p><strong>支持markdown</strong></p>
<p>Hexo 支持 GitHub Flavored Markdown 的所有功能，甚至可以整合 Octopress 的大多数插件。</p>
</li>
<li><p><strong>一键部署</strong></p>
<p>只需一条指令即可部署到 GitHub Pages, Heroku 或其他平台。</p>
</li>
<li><p><strong>可扩展性强</strong></p>
<p>强大的 API 带来无限的可能，与数种模板引擎（EJS，Pug，Nunjucks）和工具（Babel，PostCSS，Less/Sass）轻易集成</p>
</li>
</ul>
<h2 id="四、博客样式以及美学考量"><a href="#四、博客样式以及美学考量" class="headerlink" title="四、博客样式以及美学考量"></a>四、博客样式以及美学考量</h2><p>本博客的主题为Ayer，可在Hexo官网的themes中下载。<a href="https://github.com/Shen-Yu/hexo-theme-ayer">Ayer</a> 是一个干净且优雅的 Hexo 主题，文章内容美观易读，首页封面全屏平铺，扁平化设计，简洁优美。</p>
<h2 id="五、博客制作过程中遇到的问题及其解决方法"><a href="#五、博客制作过程中遇到的问题及其解决方法" class="headerlink" title="五、博客制作过程中遇到的问题及其解决方法"></a>五、博客制作过程中遇到的问题及其解决方法</h2><p>本地静态博客部署到github上时，出现无法显示的情况，问题出在无法连接到github仓库的http地址，后换为ssh连接，问题即可解决。</p>
<pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">deploy</span><span class="token punctuation">:</span>
  <span class="token key atrule">type</span><span class="token punctuation">:</span> git
  <span class="token key atrule">repository</span><span class="token punctuation">:</span> git@github.com<span class="token punctuation">:</span>Miaheeee/Miaheeee.github.io.git
  <span class="token key atrule">branch</span><span class="token punctuation">:</span> main<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

]]></content>
      <categories>
        <category>总结与反思</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
</search>
